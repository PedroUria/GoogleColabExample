{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SampleCode.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PrDoosAUah8L","colab_type":"text"},"source":["https://towardsdatascience.com/google-colab-59ad8510eb7e"]},{"cell_type":"markdown","metadata":{"id":"FNIxvanbc6Ib","colab_type":"text"},"source":["For running shell commands, do it in a code cell but with `!` before:"]},{"cell_type":"code","metadata":{"id":"ev4FR7J_awW7","colab_type":"code","outputId":"1ed532b0-2d64-4eb7-a27d-0258233ece5c","executionInfo":{"status":"ok","timestamp":1576378321816,"user_tz":300,"elapsed":4264,"user":{"displayName":"Pedro Uria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0V9wMSyZdMx8KlRJ9frThGoANZrZH12yaNxcJ=s64","userId":"01317775711083955981"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pwd\n","!ls"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/content\n","gdrive\tsample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UPKS70TDdBM8","colab_type":"text"},"source":["However, [`!cd dir`](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.05-IPython-And-Shell-Commands.ipynb) will not work. Instead, do `%cd dir`. The code below mounts my Google Drive, changes dir to a sample file I have and runs it"]},{"cell_type":"code","metadata":{"id":"kMjtdZIJa7o5","colab_type":"code","outputId":"617d5749-c740-4b0f-bc63-418ee7987abc","executionInfo":{"status":"ok","timestamp":1576379048605,"user_tz":300,"elapsed":24006,"user":{"displayName":"Pedro Uria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0V9wMSyZdMx8KlRJ9frThGoANZrZH12yaNxcJ=s64","userId":"01317775711083955981"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ls\n","\n","%cd /content/gdrive/My Drive/GitHub/GoogleColabExample/\n","!ls\n","!python example_MNIST.py"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","example_MNIST.py  MNIST  SampleCode.ipynb\n","/content/gdrive/My Drive/Colab Notebooks/Example\n","example_MNIST.py  MNIST  SampleCode.ipynb\n","Starting training loop...\n","Epoch 0 | Train Loss 0.45826, Train Acc 95.53 - Test Loss 0.15717, Test Acc 95.00\n","Epoch 1 | Train Loss 0.17889, Train Acc 96.98 - Test Loss 0.11479, Test Acc 96.40\n","Epoch 2 | Train Loss 0.13321, Train Acc 97.62 - Test Loss 0.10143, Test Acc 96.81\n","Epoch 3 | Train Loss 0.11308, Train Acc 98.13 - Test Loss 0.09121, Test Acc 97.04\n","Epoch 4 | Train Loss 0.09625, Train Acc 98.28 - Test Loss 0.08498, Test Acc 97.42\n","Epoch 5 | Train Loss 0.08843, Train Acc 98.38 - Test Loss 0.08683, Test Acc 97.39\n","Epoch 6 | Train Loss 0.08126, Train Acc 98.71 - Test Loss 0.07801, Test Acc 97.49\n","Epoch 7 | Train Loss 0.07208, Train Acc 98.94 - Test Loss 0.07439, Test Acc 97.80\n","Epoch 8 | Train Loss 0.06546, Train Acc 98.96 - Test Loss 0.07647, Test Acc 97.69\n","Epoch 9 | Train Loss 0.06289, Train Acc 99.15 - Test Loss 0.07528, Test Acc 97.68\n","Epoch 10 | Train Loss 0.05720, Train Acc 99.12 - Test Loss 0.07301, Test Acc 97.87\n","Epoch 11 | Train Loss 0.05748, Train Acc 99.29 - Test Loss 0.06897, Test Acc 97.93\n","Epoch 12 | Train Loss 0.05239, Train Acc 99.27 - Test Loss 0.07223, Test Acc 97.84\n","Epoch 13 | Train Loss 0.05025, Train Acc 99.39 - Test Loss 0.06999, Test Acc 97.85\n","Epoch 14 | Train Loss 0.04800, Train Acc 99.45 - Test Loss 0.06869, Test Acc 98.04\n","Epoch 15 | Train Loss 0.04458, Train Acc 99.51 - Test Loss 0.06717, Test Acc 98.04\n","Epoch 16 | Train Loss 0.04304, Train Acc 99.50 - Test Loss 0.07203, Test Acc 98.01\n","Epoch 17 | Train Loss 0.04191, Train Acc 99.57 - Test Loss 0.06760, Test Acc 98.24\n","Epoch 18 | Train Loss 0.03977, Train Acc 99.52 - Test Loss 0.07283, Test Acc 98.03\n","Epoch 19 | Train Loss 0.04079, Train Acc 99.58 - Test Loss 0.07185, Test Acc 98.01\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4vaM2OZ8g5xl","colab_type":"text"},"source":["Haven't been able to run two cells at one and go gpustat, but on the top right there is a bar for RAM memory, which will fill up proportionally to the GPU memory usage!\n","\n","The code below creates a txt and downloads to my local drive. Note that the created files in this session will also be in google drive."]},{"cell_type":"code","metadata":{"id":"5Rp255WmhV5i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"d2f58ecd-8113-451a-de15-992dcfd56cbf","executionInfo":{"status":"ok","timestamp":1576379398457,"user_tz":300,"elapsed":7036,"user":{"displayName":"Pedro Uria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0V9wMSyZdMx8KlRJ9frThGoANZrZH12yaNxcJ=s64","userId":"01317775711083955981"}}},"source":["!ls\n","with open(\"hoho.txt\", \"w\") as s:\n","    s.write(\"mmm\")\n","!ls\n","\n","from google.colab import files\n","files.download(\"hoho.txt\")"],"execution_count":48,"outputs":[{"output_type":"stream","text":["example_MNIST.py  MNIST  SampleCode.ipynb\n","example_MNIST.py  hoho.txt  MNIST  SampleCode.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tjScqqmyj7C3","colab_type":"text"},"source":["To simply upload a file, given that it's not on google drive"]},{"cell_type":"code","metadata":{"id":"LvqkHAD5j-8c","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":108},"outputId":"5fb1ef58-be80-4260-98cb-5e5be45d1e49","executionInfo":{"status":"ok","timestamp":1576379902582,"user_tz":300,"elapsed":18050,"user":{"displayName":"Pedro Uria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0V9wMSyZdMx8KlRJ9frThGoANZrZH12yaNxcJ=s64","userId":"01317775711083955981"}}},"source":["from google.colab import files\n","files.upload()"],"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-7de0434e-39ee-4ab6-a39f-eb9b63a9ddf6\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-7de0434e-39ee-4ab6-a39f-eb9b63a9ddf6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving example_XOR.py to example_XOR.py\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'example_XOR.py': b'# %% --------------------------------------- Imports -------------------------------------------------------------------\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport torch\\nimport torch.nn as nn\\nfrom sklearn.metrics import accuracy_score\\n\\n\\n# %% ----------------------------------- Hyper Parameters --------------------------------------------------------------\\nLR = 0.02\\nN_NEURONS = 2\\nN_EPOCHS = 10000\\nPRINT_LOSS_EVERY = 1000\\n\\n\\n# %% -------------------------------------- MLP Class ------------------------------------------------------------------\\nclass MLP(nn.Module):\\n    def __init__(self, hidden_dim):\\n        super(MLP, self).__init__()\\n        self.linear1 = nn.Linear(2, hidden_dim)\\n        self.act1 = torch.sigmoid\\n        self.linear2 = nn.Linear(hidden_dim, 2)\\n\\n    def forward(self, x):\\n        return self.linear2(self.act1(self.linear1(x)))\\n\\n\\n# %% -------------------------------------- Data Prep ------------------------------------------------------------------\\np = np.array([[0, 0], [1, 1], [0, 1], [1, 0]])\\nt = np.array([0, 0, 1, 1])\\np = torch.FloatTensor(p)\\np.requires_grad = True\\nt = torch.Tensor(t).long()\\n\\n# %% -------------------------------------- Training Prep --------------------------------------------------------------\\nmodel = MLP(N_NEURONS)\\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\\n# A classification problem is usually approached with a Categorical Cross-Entropy performance index\\ncriterion = nn.CrossEntropyLoss()  # This one combines both the loss and the Log-SoftMax output function,\\n# which computes the probabilities of each example belonging to each class\\n# This is the preferred way as its computation is more stable. Thus, there is no need to include the\\n# SoftMax/Log-SoftMax output function on the model itself\\n\\n# %% -------------------------------------- Training Loop --------------------------------------------------------------\\nfor epoch in range(N_EPOCHS):\\n    optimizer.zero_grad()\\n    logits = model(p)\\n    loss = criterion(logits, t)\\n    loss.backward()\\n    optimizer.step()\\n    if epoch % PRINT_LOSS_EVERY == 0:\\n        print(\"Epoch {} | Loss {:.5f}\".format(epoch, loss.item()))\\n\\n# %% -------------------------------------- Check Approx ---------------------------------------------------------------\\nprint(accuracy_score(t.numpy(), np.argmax(logits.detach().numpy(), axis=1))*100)\\n\\n# Gets weights and biases, and from them the DBs, and plots them\\nw, b = model.linear1.weight.detach().numpy(), model.linear1.bias.detach().numpy()\\na = np.arange(0, 1.1, 0.1)\\ndb1 = -w[0, 0]*a/w[0, 1] - b[0]/w[0, 1]\\ndb2 = -w[1, 0]*a/w[1, 1] - b[1]/w[1, 1]\\np_np = p.detach().cpu().numpy()\\nplt.scatter(p_np[:2, 0], p_np[:2, 1], marker=\"*\", c=\"b\")\\nplt.scatter(p_np[2:, 0], p_np[2:, 1], marker=\".\", c=\"b\")\\nplt.plot(a, db1, c=\"b\")\\nplt.plot(a, db2, c=\"b\")\\nplt.show()\\n'}"]},"metadata":{"tags":[]},"execution_count":50}]}]}
